{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "dataset = pd.read_csv('train.csv',index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.linear_model as lm\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import log_loss\n",
    "from neupy import algorithms, environment\n",
    "from sklearn import metrics\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = dataset.species\n",
    "\n",
    "from sklearn import preprocessing\n",
    "le = preprocessing.LabelEncoder()\n",
    "if y.dtype == object:\n",
    "    y = le.fit_transform(y)\n",
    "else:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = dataset.drop('species', axis=1)\n",
    "X = (dataset.ix[:,1:].values).astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/sklearn/linear_model/stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n",
      "                                                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 3 49 65 94 84 40 54 78 53 89 98 16 74 50 58 31 43  4 75 44 83 84 13 66\n",
      " 15  6 73 22 73 31 36 27 94 88 12 28 21 25 20 60 65 69 58 23 76 18 52 54\n",
      "  9 48 47 64 81 83 36 21 81 20 62 88 34 92 79 82 32  4 35 72 60 71 72 52\n",
      " 50 11 51 18 47  5  8 37 97 33  1 59  1 56  9 57 79 29 16 32 93 10 46 59\n",
      " 76 15 10  0 69 51 39 62  2 24 26 35 25 87  0 55 34 38 45  7 93 56 38 75\n",
      " 74 33 37 40 71 67 30 66 43 61 23 87 46 57  2 28 12 96 44 29 41 67 61 30\n",
      "  5  3  6 85 39 14 98 17 42 63 86 86 80 78 14  8 26 13 77 80 92 45 41 17\n",
      " 27 19 49 95 82 63 42 22 53 24 77 70 55 90 96 48 97 19 89 95 64 11 68 90\n",
      " 91 70 85  7 91 68]\n",
      "[0.01037408 0.00985452 0.00943888 0.01158202 0.0085934  0.00926718\n",
      " 0.00958903 0.00993012 0.00981498 0.00986098 0.01072222 0.00979255\n",
      " 0.01076085 0.00990468 0.00986265 0.00974823 0.00981153 0.00940073\n",
      " 0.01058563 0.01057633 0.01021169 0.01020361 0.00994206 0.00965784\n",
      " 0.01040188 0.00935955 0.01030709 0.0095074  0.01027142 0.00966383\n",
      " 0.01020378 0.00977169 0.01051225 0.00876883 0.01007768 0.0086112\n",
      " 0.01014807 0.00894478 0.00951794 0.00925532 0.00875842 0.01092884\n",
      " 0.01046373 0.01053889 0.00887353 0.01046184 0.01101425 0.01038952\n",
      " 0.01079594 0.01007313 0.01148289 0.01084481 0.01150141 0.01129707\n",
      " 0.01019626 0.01092085 0.01087223 0.00820562 0.01002004 0.0094441\n",
      " 0.01068503 0.00968102 0.00946381 0.00953311 0.01128878 0.01028347\n",
      " 0.00957859 0.01009097 0.01071578 0.01075536 0.00944411 0.00934882\n",
      " 0.00908058 0.01031083 0.01008259 0.01153502 0.0103454  0.00941639\n",
      " 0.00928771 0.01066721 0.00952094 0.01102376 0.00944961 0.01154458\n",
      " 0.00991551 0.01097603 0.01102154 0.01041946 0.0090607  0.00989836\n",
      " 0.01044022 0.01034475 0.01057998 0.01035403 0.01102221 0.00945545\n",
      " 0.01044817 0.0100349  0.01100343]\n",
      "[0.06405959 0.01609216 0.00177876 0.00859196 0.00619618 0.00149704\n",
      " 0.006712   0.004381   0.00647039 0.02861323 0.02472238 0.00571186\n",
      " 0.01283548 0.00557465 0.00713889 0.00467766 0.00114083 0.01403567\n",
      " 0.00762314 0.00816329 0.00467787 0.01928387 0.01563759 0.00270128\n",
      " 0.0057684  0.00743114 0.00571296 0.00303138 0.00500916 0.00276582\n",
      " 0.01019929 0.0156482  0.00792221 0.00467864 0.00195134 0.00145274\n",
      " 0.0120121  0.0020419  0.00164922 0.00452236 0.0012151  0.01121667\n",
      " 0.00979413 0.01021615 0.00341752 0.00519477 0.0166371  0.00672121\n",
      " 0.00482588 0.00471996 0.03539774 0.01619137 0.01691911 0.03413336\n",
      " 0.00460231 0.02721884 0.02304715 0.00167931 0.00346256 0.00149449\n",
      " 0.00497129 0.00812729 0.01816631 0.00865839 0.00959704 0.00985428\n",
      " 0.00232461 0.00188656 0.00556054 0.00895667 0.00212256 0.00148555\n",
      " 0.00148107 0.00410436 0.00683519 0.01648327 0.00854055 0.0045235\n",
      " 0.00212743 0.02067527 0.00180253 0.01884238 0.0049679  0.01264608\n",
      " 0.0305959  0.00978132 0.01763912 0.00380222 0.00212548 0.00443339\n",
      " 0.00768276 0.00470316 0.04623249 0.00494926 0.00788922 0.00959176\n",
      " 0.03068607 0.00753087 0.01939799]\n",
      "[0.01010476 0.01010267 0.01009955 0.01010936 0.01008598 0.01009934\n",
      " 0.01010393 0.01009088 0.01010353 0.01010002 0.01010504 0.01010234\n",
      " 0.0101049  0.010099   0.01010156 0.01009282 0.01010293 0.01008773\n",
      " 0.01010304 0.01010708 0.01010435 0.01010122 0.01010395 0.01010146\n",
      " 0.01010179 0.01009956 0.0101032  0.0101002  0.01010196 0.0100993\n",
      " 0.01009831 0.01009797 0.01009228 0.01008857 0.01010267 0.0100886\n",
      " 0.01010478 0.0100945  0.01009687 0.0100997  0.01009295 0.01010606\n",
      " 0.01009618 0.01010657 0.01007254 0.01010208 0.01010784 0.01010428\n",
      " 0.0101049  0.01009962 0.01010816 0.01010763 0.01010544 0.01010749\n",
      " 0.01010517 0.01010672 0.01010778 0.0100684  0.01010268 0.01010213\n",
      " 0.01009926 0.01010152 0.01009415 0.01010053 0.01010032 0.01010405\n",
      " 0.01009797 0.01010549 0.01010493 0.01010712 0.01010301 0.01009913\n",
      " 0.01010005 0.0101029  0.01010263 0.01010619 0.01010573 0.01009974\n",
      " 0.01010171 0.01010692 0.01010295 0.01010564 0.01010213 0.01010646\n",
      " 0.01010178 0.01010656 0.01010579 0.01010609 0.01008831 0.01009706\n",
      " 0.01010459 0.0101     0.01010362 0.01010577 0.01010571 0.01009749\n",
      " 0.01010685 0.01010113 0.01010645]\n",
      "[0.01171969 0.00772591 0.01483397 0.01663403 0.00611655 0.00813996\n",
      " 0.01361622 0.01300692 0.00956868 0.00370923 0.00939289 0.01288667\n",
      " 0.01008938 0.00937281 0.00978448 0.00875952 0.00666985 0.01120507\n",
      " 0.01045637 0.01303708 0.00952828 0.00871246 0.01166549 0.01042196\n",
      " 0.01028047 0.00984098 0.0058608  0.00810666 0.0108822  0.00945378\n",
      " 0.01044166 0.0065991  0.00885702 0.00930558 0.01002267 0.005945\n",
      " 0.01352336 0.01328182 0.01290227 0.0108115  0.01301977 0.00910505\n",
      " 0.01010992 0.0103378  0.01021459 0.01507945 0.00215995 0.01456732\n",
      " 0.01078481 0.00877523 0.01542122 0.00124006 0.01236569 0.01116686\n",
      " 0.01081826 0.0106389  0.00031546 0.00937824 0.01049006 0.0144977\n",
      " 0.01151333 0.00833141 0.01094645 0.01456305 0.00435713 0.01148237\n",
      " 0.0079536  0.00964992 0.01488413 0.01860116 0.00960336 0.01205709\n",
      " 0.00757678 0.01290037 0.00047319 0.00150876 0.01702351 0.00723498\n",
      " 0.00173066 0.01012289 0.01069547 0.01326613 0.01399499 0.00552849\n",
      " 0.00947677 0.01530039 0.01148979 0.01704287 0.00126183 0.01114952\n",
      " 0.01285906 0.00988498 0.01094577 0.007625   0.01501525 0.01248486\n",
      " 0.011384   0.00411606 0.01030995]\n",
      "[0.  0.  0.  0.8 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      " 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      " 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.2 0.  0.  0.  0.  0.  0.  0.\n",
      " 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      " 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      " 0.  0.  0.  0.  0.  0.  0.  0.  0. ]\n",
      "Log loss - Logistic regression for fold 1= 4.179430033221389/n SGD  = 2.621359444445276/n PNN = 4.593995012417452/n RFC = 3.613331255428851/n KNN = 1.077392556710472\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/scipy/optimize/linesearch.py:312: LineSearchWarning: The line search algorithm did not converge\n",
      "  warn('The line search algorithm did not converge', LineSearchWarning)\n",
      "/opt/conda/lib/python3.6/site-packages/sklearn/utils/optimize.py:195: UserWarning: Line Search failed\n",
      "  warnings.warn('Line Search failed')\n",
      "/opt/conda/lib/python3.6/site-packages/sklearn/linear_model/stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n",
      "                                                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[84 58 20 84 36 54 20  1 54 15  4 51 94 36  1 21 51 81  9 60 31 83 50 65\n",
      " 16 76 75 87 12 62 83 81  4 37 57 71 61  6 76 31 40 16 37 69 71 35 25  5\n",
      " 39  9 44 60 14 13 87 18 78 78 30 14 35 46 21  8  6 92 38 40 15 32 93 92\n",
      " 38 74 67 45 80 74 44 47 57 94 17 32 93 24 80 59 46 12 79 69  2 63 53 67\n",
      "  0  2 42 10 52 59 22 86 43 75 24 86 45 98 66 73 73 79 56 41 25 27 18 63\n",
      "  0  8 27 61 29 50 33 29 43 23 17 66 65 10 26 58 42 53 30 97 22 56 96 28\n",
      " 82 90 64 28 34 88 89 26 88 41 98 70  7 52 70 13 91 95 23 49 55 19 85 64\n",
      " 96 34 95 72 91 11 39  5 55 82 89 85 48 47  7 77 11 68 62  3 33 68 19 49\n",
      "  3 90 72 77 48 97]\n",
      "[0.0117618  0.01118657 0.01135858 0.00930418 0.01337278 0.01196179\n",
      " 0.01147491 0.00822036 0.01135431 0.01121965 0.00990436 0.00891749\n",
      " 0.00894144 0.00856057 0.00873885 0.01126217 0.01084346 0.00835944\n",
      " 0.00908811 0.01005337 0.01080273 0.00948528 0.00937902 0.01150265\n",
      " 0.00901156 0.01184977 0.00922018 0.00869058 0.00976047 0.0110604\n",
      " 0.00974083 0.01174662 0.01120597 0.01290305 0.01138552 0.01245859\n",
      " 0.01041502 0.01266589 0.01114563 0.01277431 0.01254802 0.00878245\n",
      " 0.0086897  0.00915592 0.00857108 0.01079414 0.00972262 0.00930452\n",
      " 0.00881463 0.0088943  0.00920202 0.01088712 0.00877271 0.00890067\n",
      " 0.0113768  0.01017068 0.00930624 0.00809557 0.00915086 0.01141768\n",
      " 0.00912449 0.01043548 0.00838727 0.00969763 0.01070509 0.01162679\n",
      " 0.00848148 0.00914513 0.0093549  0.00992518 0.01155962 0.01058782\n",
      " 0.01271272 0.00868255 0.01118894 0.00889065 0.01019882 0.00871341\n",
      " 0.01166967 0.01020469 0.01067924 0.00918548 0.01146307 0.00881731\n",
      " 0.01480643 0.00966764 0.00851469 0.00979497 0.00841272 0.00893207\n",
      " 0.01042823 0.00848966 0.00915957 0.00949113 0.00900771 0.00852159\n",
      " 0.00955555 0.00856222 0.00957244]\n",
      "[0.00691606 0.02346193 0.00471033 0.00191081 0.00227695 0.00677594\n",
      " 0.01179173 0.00250619 0.0263519  0.00618167 0.00224772 0.00743831\n",
      " 0.00074495 0.00137732 0.00096419 0.00608238 0.00613481 0.00071429\n",
      " 0.00659239 0.00541168 0.00353039 0.00366249 0.00119809 0.00939743\n",
      " 0.00452609 0.01346696 0.00799101 0.00351106 0.00318148 0.01443198\n",
      " 0.00178592 0.08322473 0.00193298 0.00618148 0.02266828 0.02082527\n",
      " 0.01655835 0.01242789 0.02957682 0.01679306 0.00360744 0.00130394\n",
      " 0.00206451 0.0045659  0.00048629 0.02997059 0.00201077 0.0108055\n",
      " 0.00414802 0.01195643 0.00128708 0.00916003 0.00234105 0.00123666\n",
      " 0.01088363 0.00997691 0.00170296 0.00269105 0.01027129 0.02321041\n",
      " 0.00157391 0.00558905 0.00085131 0.01601833 0.01467855 0.00829942\n",
      " 0.00056811 0.00088819 0.00518011 0.005159   0.04341819 0.01602377\n",
      " 0.08392968 0.00215541 0.00959007 0.00632644 0.00877576 0.00387595\n",
      " 0.02317413 0.01473822 0.01380314 0.00209255 0.00731202 0.00087678\n",
      " 0.09925725 0.00344942 0.00201982 0.00170652 0.00456773 0.00166357\n",
      " 0.00601999 0.00275827 0.000956   0.01058796 0.00314488 0.00237586\n",
      " 0.00690605 0.00685871 0.00168614]\n",
      "[0.01010822 0.01010641 0.01010515 0.01010378 0.01009848 0.01010548\n",
      " 0.01010868 0.01008486 0.01010748 0.0101037  0.01010365 0.01009979\n",
      " 0.01010026 0.01009553 0.01009887 0.01009719 0.01010584 0.01008235\n",
      " 0.01009869 0.01010594 0.01010618 0.01009787 0.01010232 0.01010627\n",
      " 0.01009825 0.01010628 0.01010063 0.01009811 0.01010161 0.010103\n",
      " 0.01009824 0.01010519 0.01009615 0.01009994 0.01010346 0.01010023\n",
      " 0.01010547 0.01010474 0.01010134 0.01010802 0.01010355 0.01009998\n",
      " 0.01009155 0.01010275 0.01007448 0.01010248 0.01010474 0.01010148\n",
      " 0.0100998  0.01009741 0.01010235 0.01010748 0.01009803 0.01010114\n",
      " 0.01010846 0.01010472 0.01010399 0.01006341 0.01010001 0.01010725\n",
      " 0.01009485 0.010104   0.01009171 0.01010096 0.01010068 0.01010742\n",
      " 0.01009535 0.01010307 0.01010138 0.01010521 0.01010841 0.01010312\n",
      " 0.01010824 0.01009735 0.0101054  0.01009968 0.01010549 0.0100981\n",
      " 0.01010805 0.01010567 0.01010607 0.01010047 0.0101071  0.01009909\n",
      " 0.01011298 0.01010375 0.01009886 0.01010488 0.01008802 0.01009399\n",
      " 0.01010532 0.01009422 0.01009998 0.01010372 0.01010029 0.01009431\n",
      " 0.01010441 0.01009688 0.01010283]\n",
      "[0.00466975 0.00857958 0.01135977 0.00777239 0.00655935 0.01171973\n",
      " 0.00523305 0.00765264 0.00650893 0.00717123 0.00908242 0.00555169\n",
      " 0.00849146 0.0052374  0.01120951 0.00598137 0.00863059 0.00678532\n",
      " 0.00571166 0.00853839 0.00648752 0.01079672 0.00483154 0.00697711\n",
      " 0.00481671 0.00785051 0.00992113 0.00880122 0.00828022 0.00693917\n",
      " 0.00750549 0.00925921 0.01107421 0.00854134 0.01482769 0.00967034\n",
      " 0.01851361 0.01148586 0.01580294 0.0109833  0.03133422 0.00929402\n",
      " 0.01017029 0.00761624 0.01258575 0.00654381 0.003943   0.01500433\n",
      " 0.00772505 0.00740929 0.01927069 0.00513405 0.00771582 0.00820435\n",
      " 0.00840947 0.00694303 0.0028318  0.03027362 0.0273351  0.00684678\n",
      " 0.00521566 0.00720435 0.00652188 0.00569127 0.00488942 0.00763543\n",
      " 0.01150873 0.0043578  0.00845172 0.00828154 0.00654477 0.00713208\n",
      " 0.00684114 0.0103233  0.00255073 0.0050908  0.03192407 0.00989458\n",
      " 0.00196372 0.00640554 0.00601936 0.00807587 0.00937061 0.00928827\n",
      " 0.11524024 0.01124556 0.00735732 0.02248743 0.00326847 0.00740734\n",
      " 0.00734753 0.00830478 0.00555335 0.00641873 0.00726837 0.00428737\n",
      " 0.00523663 0.00842591 0.00856559]\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0.]\n",
      "Log loss - Logistic regression for fold 2= 4.1698608723553745/n SGD  = 2.5867931644197673/n PNN = 4.5939700349975965/n RFC = 3.6832643291407803/n KNN = 1.009189971059486\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/scipy/optimize/linesearch.py:312: LineSearchWarning: The line search algorithm did not converge\n",
      "  warn('The line search algorithm did not converge', LineSearchWarning)\n",
      "/opt/conda/lib/python3.6/site-packages/sklearn/utils/optimize.py:195: UserWarning: Line Search failed\n",
      "  warnings.warn('Line Search failed')\n",
      "/opt/conda/lib/python3.6/site-packages/sklearn/linear_model/stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n",
      "                                                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[20 84 20  1 84 51 60 51 71 78 15 60 21 36 31 16 83 15 76 16 83 12  1 36\n",
      "  9  6 31 37 32 21 44 45 40 80 35 25 35 59 37  6 78 54 93 46 79 59  0 38\n",
      " 38 17 25 86 46 24 93  8 32 14 10 94  8 44 74 43 57 24 61 92 45 61 41 14\n",
      " 28 26 74 42 66 22 53 22 40 28 70 50 29  9 18 63 30 66 88 63 13 27 29 65\n",
      " 71  2 75 67 81 57 81 55 54 86 96 10 58 52 69 39 95 39 82 52 55 91 12 96\n",
      " 82 64 73 23 75 43  2 11 47 53 56 62 80 56 30 88 73 76  5 87 62 47 42 23\n",
      " 87 94 69  5 79 50 92  4 33 49 26 18 13 89 67 41 11  0 98 85 17 49 89 58\n",
      "  4 34 65 19 68 72  7 90  3 27 95 98 72 19  7  3 91 77 64 70 85 90 68 34\n",
      " 97 97 77 48 33 48]\n",
      "[0.01176449 0.01073684 0.00914775 0.01030798 0.01047412 0.00943801\n",
      " 0.00998727 0.00915326 0.01059091 0.01177485 0.01019096 0.01059224\n",
      " 0.01144043 0.00993704 0.01126456 0.00854759 0.01133816 0.00929305\n",
      " 0.0113339  0.01001592 0.01343263 0.00936577 0.01021717 0.01128584\n",
      " 0.01052747 0.0101719  0.01033123 0.00952389 0.0093367  0.00974777\n",
      " 0.00880949 0.00956669 0.00863252 0.00855374 0.00910449 0.01030827\n",
      " 0.00962209 0.0099204  0.01012282 0.00982123 0.00910391 0.01123274\n",
      " 0.0084718  0.01048896 0.00882204 0.00929235 0.00997643 0.01024829\n",
      " 0.011073   0.01042944 0.009645   0.01047828 0.00922469 0.01005577\n",
      " 0.01070879 0.01017698 0.00989416 0.00822382 0.00957169 0.01095266\n",
      " 0.01028059 0.01009778 0.00907047 0.01037194 0.01017451 0.0108698\n",
      " 0.00892892 0.00902823 0.00964104 0.00986972 0.01053716 0.01077054\n",
      " 0.00925645 0.00921117 0.01224577 0.01007179 0.01047726 0.00943612\n",
      " 0.01045157 0.01011281 0.01086678 0.00925206 0.0106943  0.00924363\n",
      " 0.01056856 0.01080053 0.00975815 0.00975112 0.00864502 0.01047073\n",
      " 0.0098984  0.01167695 0.00981073 0.0119336  0.01044155 0.01070257\n",
      " 0.01019263 0.01070473 0.00988211]\n",
      "[0.04883135 0.00274249 0.0006689  0.00315394 0.00266752 0.0019991\n",
      " 0.0041458  0.00324086 0.00948237 0.01395836 0.00179769 0.00366494\n",
      " 0.00893066 0.00301992 0.0217204  0.00055785 0.0350933  0.00475563\n",
      " 0.02289976 0.00216579 0.0440907  0.00061419 0.05575639 0.0120168\n",
      " 0.02447774 0.01897586 0.00610881 0.00151576 0.00329398 0.00165805\n",
      " 0.00058671 0.00202941 0.0051843  0.00191761 0.00251779 0.00278531\n",
      " 0.00148525 0.01197173 0.00802403 0.00987836 0.00244653 0.06063577\n",
      " 0.00070637 0.02475008 0.00121965 0.00234452 0.00280338 0.00681657\n",
      " 0.00447064 0.00974634 0.02952503 0.01072447 0.00214519 0.01376777\n",
      " 0.01766034 0.00355663 0.00387577 0.00071899 0.02703884 0.00586833\n",
      " 0.00905985 0.0013152  0.00105676 0.00303111 0.02212138 0.0418824\n",
      " 0.00897439 0.00069476 0.02507097 0.00612005 0.00650411 0.00418264\n",
      " 0.00245778 0.00042966 0.02989394 0.0047858  0.00522164 0.00232421\n",
      " 0.04004389 0.00624484 0.0337717  0.00097353 0.00265638 0.00205963\n",
      " 0.00488341 0.00894479 0.00233649 0.00412395 0.00127122 0.00433478\n",
      " 0.00287466 0.01020158 0.01139265 0.01619823 0.00731915 0.00196081\n",
      " 0.00425604 0.01383445 0.00398454]\n",
      "[0.01010836 0.01010539 0.01009831 0.01010677 0.01009123 0.01009914\n",
      " 0.01010506 0.01008907 0.01010562 0.01010467 0.01010382 0.01010469\n",
      " 0.01010675 0.01009796 0.01010538 0.01008739 0.01010712 0.0100857\n",
      " 0.01010474 0.010106   0.01011179 0.01009716 0.01010456 0.0101055\n",
      " 0.01010262 0.01010221 0.01010401 0.01010052 0.01009946 0.01009929\n",
      " 0.01009324 0.01009647 0.01008655 0.01008862 0.01009787 0.01009358\n",
      " 0.01010371 0.01009789 0.0100989  0.01010088 0.01009512 0.01010711\n",
      " 0.0100899  0.01010668 0.01007415 0.01009822 0.01010525 0.01010467\n",
      " 0.01010601 0.01010084 0.01010385 0.01010666 0.01009984 0.01010491\n",
      " 0.01010701 0.01010452 0.01010535 0.01006386 0.01010113 0.01010614\n",
      " 0.0100982  0.01010289 0.01009393 0.0101029  0.010099   0.01010541\n",
      " 0.01009705 0.01010281 0.01010224 0.01010508 0.01010613 0.01010366\n",
      " 0.01009977 0.01009907 0.01010783 0.01010281 0.01010642 0.01009977\n",
      " 0.0101052  0.01010532 0.010107   0.0101017  0.01010538 0.01010028\n",
      " 0.0101044  0.01010654 0.01010303 0.01010471 0.01008766 0.01009817\n",
      " 0.01010333 0.01010283 0.01010223 0.01010971 0.01010482 0.01010103\n",
      " 0.01010624 0.01010275 0.01010346]\n",
      "[0.01382056 0.00498333 0.007848   0.01358688 0.02060683 0.00495849\n",
      " 0.00763954 0.01215972 0.01055017 0.02978417 0.00773685 0.0601846\n",
      " 0.01076742 0.01638669 0.00628972 0.01203774 0.00377194 0.01294031\n",
      " 0.01240885 0.01019195 0.01520376 0.00611021 0.01290953 0.0129562\n",
      " 0.01056554 0.01162045 0.00777415 0.0051662  0.01072759 0.01274633\n",
      " 0.01495798 0.00594437 0.00659311 0.01067837 0.00792165 0.00521198\n",
      " 0.00943053 0.00959912 0.00851452 0.00863778 0.00817568 0.00805635\n",
      " 0.00898553 0.01092136 0.00665284 0.01327521 0.00357703 0.01071184\n",
      " 0.00995547 0.01679813 0.01192285 0.00254045 0.00875831 0.01285203\n",
      " 0.01041208 0.01040055 0.00265811 0.00569528 0.00569765 0.00848227\n",
      " 0.01455633 0.00866346 0.01173906 0.00906519 0.0050888  0.01141369\n",
      " 0.0060123  0.01024473 0.01021282 0.00998467 0.01450627 0.0065464\n",
      " 0.00528839 0.01484054 0.00243002 0.00403863 0.00701536 0.00608118\n",
      " 0.00255105 0.01215267 0.01059224 0.01078711 0.00474723 0.00597135\n",
      " 0.00727652 0.01057302 0.01415466 0.00725762 0.00304381 0.00727968\n",
      " 0.01016132 0.00968837 0.01210115 0.01090354 0.01298051 0.01123483\n",
      " 0.01085837 0.00513073 0.01437419]\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0.]\n",
      "Log loss - Logistic regression for fold 3= 4.181611604160732/n SGD  = 2.7965722989339854/n PNN = 4.594001102867946/n RFC = 3.5949080996344187/n KNN = 1.7573256843191358\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/scipy/optimize/linesearch.py:312: LineSearchWarning: The line search algorithm did not converge\n",
      "  warn('The line search algorithm did not converge', LineSearchWarning)\n",
      "/opt/conda/lib/python3.6/site-packages/sklearn/utils/optimize.py:195: UserWarning: Line Search failed\n",
      "  warnings.warn('Line Search failed')\n",
      "/opt/conda/lib/python3.6/site-packages/sklearn/linear_model/stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n",
      "                                                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[84 51 15 51 36 20 21 37 31  8 16 84 46  8 36 44 16 40  6 60 60 28 31 83\n",
      " 22 66 32 37 61 63 35 25 24 59 28 54 74 74  9 40 46 21 76 83 57 88 96 44\n",
      " 82 63 24 54 43 12  5 52  1 73 78 47 69 18 29 53  1 45  9 13  2 66 59 79\n",
      "  6 43 26 12 71 93 39 42 15 38 55 93 88 55 17 94 57 92 81 26 89 49 89 30\n",
      " 58  4  4 76 71 13 62 61 25 18 62 14 78 65 50 58 38 87 50  7 69 23 65 41\n",
      " 92 20 91 29 67  0 35 98 91 53 39 85 96 17  7 11 56 90 79 45 64 41 19 11\n",
      " 10 95 10 42 68 14 22 27 98 72  3 19 75 81 67 56 47 75 94 95 32 52 87  3\n",
      " 73 64 49 68 77 82  2 30 23 33 34 33 27 34 86  0 80 70 77 48  5 97 80 85\n",
      " 90 97 70 48 72 86]\n",
      "[0.01139749 0.0109956  0.01030068 0.00965661 0.01033061 0.0112763\n",
      " 0.01136312 0.0086026  0.010867   0.01085763 0.01000317 0.0094023\n",
      " 0.00971531 0.00912822 0.00910404 0.01041482 0.01094991 0.00881575\n",
      " 0.00909994 0.01051081 0.01124109 0.00945839 0.00940507 0.01098286\n",
      " 0.00886063 0.0101686  0.00954987 0.00857235 0.00930816 0.0111118\n",
      " 0.00990944 0.01150141 0.01208394 0.01091694 0.01095787 0.00963463\n",
      " 0.01005768 0.01275206 0.01043868 0.01137419 0.01038083 0.00940499\n",
      " 0.00888324 0.009164   0.00856738 0.01151345 0.01003852 0.0097502\n",
      " 0.00926279 0.0093825  0.00942007 0.01192149 0.00901381 0.0092193\n",
      " 0.01106947 0.01085692 0.00963687 0.00792239 0.0095662  0.01181242\n",
      " 0.00950639 0.01022916 0.00838537 0.01000825 0.01111625 0.01176203\n",
      " 0.0084883  0.00925056 0.0095474  0.01043619 0.01119522 0.01084716\n",
      " 0.01191828 0.00867607 0.01137426 0.00905513 0.01045886 0.00854946\n",
      " 0.0117425  0.0104081  0.01112335 0.0093346  0.01177025 0.00923556\n",
      " 0.01576155 0.01033271 0.00870019 0.01019604 0.00825087 0.00918682\n",
      " 0.01100341 0.0087776  0.00935744 0.01016597 0.00957281 0.00883196\n",
      " 0.00941313 0.00842623 0.00974022]\n",
      "[0.00483198 0.00997363 0.00234812 0.00626899 0.00562291 0.00793857\n",
      " 0.01136148 0.00207878 0.00374492 0.00964866 0.01616003 0.01231742\n",
      " 0.01177841 0.00434576 0.0051079  0.00350404 0.01012256 0.00079637\n",
      " 0.00157036 0.1096002  0.04496493 0.00152484 0.01053045 0.00174539\n",
      " 0.00096014 0.00179223 0.00260904 0.0043279  0.00118045 0.00454509\n",
      " 0.00477258 0.01581377 0.00432093 0.00110877 0.01174761 0.00198055\n",
      " 0.01199882 0.00797255 0.00138426 0.01932326 0.00500476 0.0052514\n",
      " 0.00334435 0.00300438 0.00103261 0.01316159 0.00826615 0.00588192\n",
      " 0.00481312 0.00160518 0.00170292 0.00653238 0.00145673 0.00347976\n",
      " 0.03212494 0.00881161 0.00782523 0.0012051  0.00142688 0.06301012\n",
      " 0.00882639 0.00273555 0.00061538 0.00982242 0.00332511 0.0275963\n",
      " 0.00473353 0.0015925  0.0184644  0.00585284 0.00448829 0.00443339\n",
      " 0.02287867 0.00162737 0.01546707 0.00114256 0.00304383 0.00049473\n",
      " 0.00621513 0.00380652 0.00215626 0.00914848 0.00277784 0.02013398\n",
      " 0.16839875 0.0307376  0.00104816 0.00448169 0.00158648 0.00670452\n",
      " 0.00283959 0.00161848 0.00193196 0.00681648 0.00486117 0.00425158\n",
      " 0.00254664 0.00110031 0.00703226]\n",
      "[0.01010754 0.01010589 0.0101027  0.01010481 0.01009097 0.01010414\n",
      " 0.01010832 0.01008862 0.01010638 0.01010223 0.0101032  0.0101018\n",
      " 0.01010215 0.01009628 0.01009953 0.01009364 0.01010624 0.01008448\n",
      " 0.01009953 0.01010706 0.01010734 0.0100978  0.01010229 0.01010475\n",
      " 0.01009769 0.01010188 0.01010144 0.01009837 0.01009963 0.010103\n",
      " 0.0100972  0.01010105 0.01009566 0.01009514 0.01010269 0.01009252\n",
      " 0.0101048  0.01010481 0.01009986 0.01010483 0.01009914 0.01010192\n",
      " 0.01009281 0.01010304 0.01007517 0.01010475 0.01010548 0.01010239\n",
      " 0.01010078 0.01009921 0.01010327 0.01010981 0.01009889 0.01010214\n",
      " 0.01010775 0.01010667 0.01010466 0.0100619  0.01010074 0.01010808\n",
      " 0.01009505 0.01010339 0.01009135 0.01010201 0.01010137 0.01010814\n",
      " 0.01009594 0.01010339 0.01010146 0.0101065  0.01010771 0.01010326\n",
      " 0.01010676 0.0100978  0.01010631 0.01010002 0.01010614 0.01009739\n",
      " 0.01010794 0.0101062  0.01010709 0.01010112 0.01010809 0.01010026\n",
      " 0.01011446 0.01010515 0.01009935 0.01010576 0.01008824 0.0100967\n",
      " 0.01010643 0.01009506 0.01010034 0.01010552 0.0101019  0.01009608\n",
      " 0.01010403 0.01009629 0.01010315]\n",
      "[0.01047664 0.00341441 0.00630887 0.00871033 0.00494552 0.01003683\n",
      " 0.00695636 0.00907442 0.00835995 0.00407554 0.00653765 0.00770833\n",
      " 0.0100035  0.00687053 0.007967   0.00619366 0.00718598 0.00624455\n",
      " 0.00713113 0.00540049 0.01073056 0.01065024 0.00647595 0.01012919\n",
      " 0.00768109 0.00835584 0.01043233 0.0090775  0.01052514 0.00982628\n",
      " 0.00793536 0.01059283 0.01068152 0.01225518 0.02773165 0.00833647\n",
      " 0.00892635 0.00708198 0.00601928 0.0071051  0.00477727 0.00751424\n",
      " 0.00805126 0.00805222 0.00991941 0.09242911 0.00690892 0.00385869\n",
      " 0.00595474 0.01047126 0.00915323 0.00857182 0.01006608 0.01203795\n",
      " 0.01161101 0.01321241 0.00772368 0.00491835 0.01242468 0.00570908\n",
      " 0.00761354 0.00506265 0.01505686 0.0068675  0.00593521 0.0114074\n",
      " 0.00984639 0.00726507 0.01153143 0.03133612 0.01199587 0.00851098\n",
      " 0.00970793 0.01126347 0.00341346 0.0062962  0.00539683 0.00810757\n",
      " 0.00583988 0.00771694 0.0084686  0.00926696 0.01182954 0.00751846\n",
      " 0.07203187 0.00503249 0.00860824 0.00342954 0.00199291 0.0082551\n",
      " 0.00787471 0.00825853 0.01038183 0.01245403 0.00909258 0.00695565\n",
      " 0.00687573 0.00821098 0.00577402]\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0.]\n",
      "Log loss - Logistic regression for fold 4= 4.17900683061041/n SGD  = 2.705016271502837/n PNN = 4.593993816866834/n RFC = 3.592108247115864/n KNN = 1.4037586841085599\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/scipy/optimize/linesearch.py:312: LineSearchWarning: The line search algorithm did not converge\n",
      "  warn('The line search algorithm did not converge', LineSearchWarning)\n",
      "/opt/conda/lib/python3.6/site-packages/sklearn/utils/optimize.py:195: UserWarning: Line Search failed\n",
      "  warnings.warn('Line Search failed')\n",
      "/opt/conda/lib/python3.6/site-packages/sklearn/linear_model/stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n",
      "Time: 0:00:00 | 50%|######################                      | ETA:  0:00:00"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[51 16 36  8 84 36 28 37 84 44 24 46 44 83  1 15 40 55 60 74 21 54 16 12\n",
      " 37 60 93 83 61 76 18  9  9 31 24 96 39 74 62 28 96  7  7 93 41 58 12 71\n",
      " 91 43 61 20 63 26 11 31 57 62 66 19 13 75 50 32 47 43 94 68 47 66 35 88\n",
      " 68 87 54 79 67 65 18  4 26 30 52 29 67 95 39 25 58 35 27 17 38 91 13 23\n",
      " 79 77 22 49 98 46 63 53 20 25 78 10 65 33 41 98 71 95 52  3 29 69 51 27\n",
      " 22 34  6 21 89 17 97 72 80 10 57 64 92 38 15 73 87 73 48 42 82 33 56  3\n",
      " 42  1 53 55 90 19  6 30 64 49  2  8 45 76 92  0 23 69 59 80 90 32  5 59\n",
      " 85 89 94 45 48 86 81 14  4 77 56 82  2 85 70 88  0 75 14 86 81 97 70 72\n",
      " 34 40  5 11 78 50]\n",
      "[0.01014771 0.0102623  0.00974067 0.01110505 0.00868414 0.01013696\n",
      " 0.01071783 0.00824164 0.00939504 0.01032492 0.00963232 0.0088003\n",
      " 0.00958669 0.00874615 0.00887501 0.0108651  0.00944835 0.00856222\n",
      " 0.0098413  0.00996689 0.01060341 0.0098314  0.00997447 0.01051776\n",
      " 0.01020626 0.00974867 0.00928704 0.00928227 0.01069564 0.01199727\n",
      " 0.01117814 0.01033172 0.01387689 0.01054602 0.01093582 0.00875712\n",
      " 0.0106974  0.01150571 0.01040467 0.01080447 0.0104944  0.00969235\n",
      " 0.0095373  0.00982918 0.00868548 0.01126011 0.01048549 0.00947198\n",
      " 0.00942169 0.00896302 0.01059589 0.01275858 0.00960769 0.00986551\n",
      " 0.01068039 0.0114389  0.01036057 0.00846457 0.00963874 0.01050971\n",
      " 0.01044208 0.0106147  0.00905817 0.01008164 0.0129818  0.01082021\n",
      " 0.00894065 0.00997475 0.00993393 0.01115407 0.01040276 0.01020123\n",
      " 0.0102155  0.00903192 0.01061032 0.0098553  0.01092292 0.00924051\n",
      " 0.01042831 0.01095185 0.01046559 0.00993514 0.01078037 0.00983389\n",
      " 0.01208032 0.01065951 0.00920383 0.01055558 0.0087612  0.00888427\n",
      " 0.0117968  0.00926131 0.00919082 0.00980874 0.00938115 0.00848809\n",
      " 0.01033982 0.00926794 0.00941875]\n",
      "[0.01220005 0.00292104 0.00182981 0.00701648 0.00179719 0.00210294\n",
      " 0.0049149  0.00037292 0.00122252 0.00518361 0.0024546  0.00134667\n",
      " 0.02657938 0.0013087  0.00619452 0.00485349 0.00141074 0.00073977\n",
      " 0.05937849 0.00778321 0.05054157 0.002094   0.01696815 0.00178763\n",
      " 0.00605998 0.01548251 0.00231552 0.00543044 0.00772651 0.01745486\n",
      " 0.00844683 0.0067704  0.0167015  0.00317661 0.00399371 0.00154541\n",
      " 0.00902702 0.00342637 0.00462563 0.00290311 0.00942261 0.00895587\n",
      " 0.01407694 0.01795159 0.00143434 0.0083295  0.0095927  0.0015353\n",
      " 0.00678343 0.00085681 0.01161315 0.06812119 0.00986277 0.01764076\n",
      " 0.01404845 0.01585999 0.01623593 0.00128697 0.00255433 0.00731429\n",
      " 0.01030161 0.03121349 0.00630715 0.00726376 0.01577866 0.02365306\n",
      " 0.00156726 0.00903909 0.00405288 0.01177327 0.00280724 0.01071412\n",
      " 0.00391422 0.00131614 0.00631731 0.00686885 0.00803371 0.00329429\n",
      " 0.0164802  0.00590844 0.00687752 0.00238857 0.03385083 0.0119426\n",
      " 0.07589756 0.00738861 0.00238352 0.01130468 0.00980471 0.00068197\n",
      " 0.01836264 0.02742378 0.00215182 0.00210451 0.00413894 0.00333916\n",
      " 0.00145465 0.0060892  0.00424426]\n",
      "[0.01010439 0.01010427 0.01010036 0.01010855 0.01008625 0.01010088\n",
      " 0.01010685 0.01008644 0.01010246 0.01010119 0.01010228 0.01009979\n",
      " 0.01010181 0.01009493 0.01009864 0.0100955  0.01010201 0.01008379\n",
      " 0.01010115 0.0101055  0.01010528 0.01009982 0.01010411 0.01010384\n",
      " 0.0101018  0.01010101 0.01010057 0.01009995 0.01010324 0.0101048\n",
      " 0.01010074 0.01009939 0.01010007 0.01009524 0.01010306 0.01009017\n",
      " 0.01010622 0.01010213 0.01009938 0.01010388 0.01009905 0.01010259\n",
      " 0.01009411 0.01010455 0.01007513 0.01010357 0.01010697 0.01010201\n",
      " 0.01010178 0.01009745 0.01010619 0.01011172 0.01010032 0.01010386\n",
      " 0.0101065  0.01010812 0.01010662 0.010065   0.01010145 0.0101048\n",
      " 0.01009817 0.01010408 0.01009303 0.01010162 0.01010404 0.01010557\n",
      " 0.01009619 0.01010548 0.01010325 0.01010835 0.01010591 0.01010167\n",
      " 0.01010258 0.01009869 0.01010418 0.01010217 0.01010724 0.01009951\n",
      " 0.01010576 0.01010762 0.01010525 0.01010299 0.01010552 0.01010285\n",
      " 0.0101072  0.01010619 0.01010073 0.01010662 0.01008697 0.01009432\n",
      " 0.01010827 0.01009677 0.01009959 0.01010436 0.01010129 0.01009454\n",
      " 0.0101066  0.01009856 0.01010275]\n",
      "[0.01019792 0.00659461 0.0041141  0.00577187 0.00542877 0.00956961\n",
      " 0.0070039  0.00351979 0.00991386 0.00589318 0.00976987 0.00522537\n",
      " 0.01077252 0.00814078 0.00668673 0.00780191 0.00850667 0.00573659\n",
      " 0.00587281 0.00653443 0.00873078 0.01755945 0.00813149 0.00687878\n",
      " 0.00556825 0.00854311 0.00859857 0.00753574 0.00844002 0.01034065\n",
      " 0.00482104 0.0178262  0.00776904 0.00977668 0.00375892 0.00599925\n",
      " 0.00378541 0.00549076 0.00585867 0.00641564 0.00402616 0.00620016\n",
      " 0.00651871 0.00333755 0.0129529  0.00368923 0.02817763 0.00332661\n",
      " 0.00577643 0.01086809 0.00541484 0.11667917 0.00792081 0.01017259\n",
      " 0.01138926 0.01151049 0.02477102 0.00375643 0.00351566 0.00481189\n",
      " 0.00725984 0.00760295 0.00884999 0.00377303 0.02753087 0.01066923\n",
      " 0.01039316 0.00627325 0.0097667  0.00707236 0.0094655  0.00549363\n",
      " 0.00900161 0.00361033 0.03483752 0.01194205 0.0051166  0.0081748\n",
      " 0.0187882  0.00945929 0.00680487 0.00644396 0.00373637 0.03592766\n",
      " 0.02349247 0.00467057 0.00717899 0.00452451 0.03086914 0.00250704\n",
      " 0.00525847 0.00517743 0.0071225  0.00883211 0.00628707 0.00409801\n",
      " 0.0047007  0.0203204  0.00726742]\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0.]\n",
      "Log loss - Logistic regression for fold 5= 4.1701799318043875/n SGD  = 2.6737701400866567/n PNN = 4.593970864959238/n RFC = 3.510191492961435/n KNN = 1.1871175798913598\n",
      "Average log loss - Logistic Regression is 4.176017854430459\n",
      "Average log loss - PNN classifier is 4.593986166421813\n",
      "Average accuracy score using lr classifier = nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                               \r",
      "/opt/conda/lib/python3.6/site-packages/numpy/core/fromnumeric.py:2957: RuntimeWarning: Mean of empty slice.\n",
      "  out=out, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "#Splitting data to 5 sets and performing multinomial regression on each\n",
    "skf = StratifiedKFold(n_splits=5)\n",
    "skf.get_n_splits(X, y)\n",
    "\n",
    "StratifiedKFold(n_splits=5, random_state=None, shuffle=False)\n",
    "\n",
    "log_loss_lr_total = []\n",
    "log_loss_pnn_total = []\n",
    "accuracy_score = []\n",
    "i=0\n",
    "environment.reproducible()\n",
    "lgr = lm.LogisticRegression(multi_class = 'multinomial', solver = 'newton-cg')\n",
    "pnn = algorithms.PNN(std=10, verbose=False)\n",
    "sgd = lm.SGDClassifier(loss='log')\n",
    "rfc = RandomForestClassifier(max_depth=3, random_state=0)\n",
    "knn = KNeighborsClassifier(n_neighbors=5)\n",
    "\n",
    "for train_index, test_index in skf.split(X, y):    \n",
    "    X_train = []\n",
    "    X_test = []\n",
    "    X_train, X_test, y_train, y_test = (X[train_index], X[test_index], y[train_index], y[test_index])\n",
    "    \n",
    "    # Multinomial linear regression\n",
    "    lgr.fit(X_train, y_train)\n",
    "    y_predicted = lgr.predict_proba(X_test)\n",
    "    logLoss = log_loss(y_test, y_predicted)\n",
    "    log_loss_lr_total.append(logLoss)\n",
    "    \n",
    "    # Stochastic Gradient Descent\n",
    "    sgd.fit(X_train, y_train)\n",
    "    y_predicted_sgd = sgd.predict_proba(X_test)\n",
    "    logLossSGD = log_loss(y_test, y_predicted_sgd)\n",
    "    \n",
    "    #PNN classifier\n",
    "    pnn.train(X_train, y_train)\n",
    "    y_predicted_pnn = pnn.predict_proba(X_test)\n",
    "    logLossPNN = log_loss(y_test, y_predicted_pnn)\n",
    "    log_loss_pnn_total.append(logLossPNN)\n",
    "    \n",
    "    #Random Forest Classifier\n",
    "    rfc.fit(X_train,y_train)\n",
    "    y_predicted_rfc = rfc.predict_proba(X_test)\n",
    "    logLossRFC = log_loss(y_test, y_predicted_rfc)\n",
    "    \n",
    "    #KNN classifier\n",
    "    knn.fit(X_train, y_train)\n",
    "    y_predicted_knn = knn.predict_proba(X_test)\n",
    "    logLossKNN = log_loss(y_test, y_predicted_knn)\n",
    "    \n",
    "    #print(y_test)\n",
    "    #print(y_predicted[0])\n",
    "    #print(y_predicted_sgd[0])\n",
    "    #print(y_predicted_pnn[0])\n",
    "    #print(y_predicted_rfc[0])\n",
    "    #print(y_predicted_knn[0])\n",
    "\n",
    "    i += 1\n",
    "    print(\"Log loss - Logistic regression for fold \" + str(i) + \"= \" + str(logLoss) + \"/n SGD  = \" + str(logLossSGD) + \"/n PNN = \" +str(logLossPNN)\n",
    "         + \"/n RFC = \" + str(logLossRFC) + \"/n KNN = \" + str(logLossKNN))\n",
    "    \n",
    "    \n",
    "    #accuracy_score.append(metrics.accuracy_score(y_test, y_predicted_pnn))\n",
    "    \n",
    "log_loss_mean = np.mean(log_loss_lr_total)\n",
    "\n",
    "#print(\"Average log loss - Logistic Regression is \" + str(log_loss_mean))\n",
    "log_loss_pnn_mean = np.mean(log_loss_pnn_total)\n",
    "#print(\"Average log loss - PNN classifier is \" + str(log_loss_pnn_mean))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Time: 0:00:00 | 75%|#################################           | ETA:  0:00:00"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log loss - Logistic regression for fold 5= 4.155838008601256/n SGD  = 2.58176208834954/n PNN = 4.593970864959238/n RFC = 3.409768578504182/n KNN = 0.47876888110993376\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                               \r"
     ]
    }
   ],
   "source": [
    "#Predicting the value for the test data\n",
    "\n",
    "test_dataset = pd.read_csv('test.csv',index_col=0)\n",
    "y_test_dataset = dataset.species\n",
    "\n",
    "from sklearn import preprocessing\n",
    "le = preprocessing.LabelEncoder()\n",
    "if y_test_dataset.dtype == object:\n",
    "    y_test_dataset = le.fit_transform(y)\n",
    "else:\n",
    "    pass\n",
    "\n",
    "X_test_dataset = dataset.drop('species', axis=1)\n",
    "X_test_dataset = (dataset.ix[:,1:].values).astype('float32')\n",
    "\n",
    "y_pred_test = lgr.predict_proba(X_test_dataset)\n",
    "log_loss_test = log_loss(y_test_dataset,y_pred_test)\n",
    "y_pred_sgd_test = sgd.predict_proba(X_test_dataset)\n",
    "log_loss_sgd_test = log_loss(y_test_dataset, y_pred_sgd_test)\n",
    "y_pred_test_knn = knn.predict_proba(X_test_dataset)\n",
    "log_loss_knn_test = log_loss(y_test_dataset,y_pred_test_knn)\n",
    "y_predicted_test_pnn = pnn.predict_proba(X_test_dataset)\n",
    "log_loss_pnn_test = log_loss(y_test_dataset, y_predicted_test_pnn)\n",
    "y_predicted_rfc_test = rfc.predict_proba(X_test_dataset)\n",
    "log_loss_rfc_test = log_loss(y_test_dataset, y_predicted_rfc_test)\n",
    "print(\"Log loss - Logistic regression for fold \" + str(i) + \"= \" + str(log_loss_test) + \"/n SGD  = \" + str(log_loss_sgd_test) + \"/n PNN = \" +str(logLossPNN)\n",
    "         + \"/n RFC = \" + str(log_loss_rfc_test) + \"/n KNN = \" + str(log_loss_knn_test))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
