{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import unittest\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from video2tfrecord import convert_videos_to_tfrecord\n",
    "from tensorflow.python.platform import gfile\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_and_decode(filename_queue, n_frames):\n",
    "    \"\"\"Creates one image sequence\"\"\"\n",
    "    outputPath = 'output/%d.jpg'\n",
    "    reader = tf.TFRecordReader()\n",
    "    _, serialized_example = reader.read(filename_queue)\n",
    "\n",
    "    image_seq = []\n",
    "\n",
    "    if n_frames == 'all':\n",
    "        n_frames = 354 \n",
    "\n",
    "    for image_count in range(n_frames):\n",
    "        path = 'blob' + '/' + str(image_count)\n",
    "\n",
    "        feature_dict = {path: tf.FixedLenFeature([], tf.string),\n",
    "                        'height': tf.FixedLenFeature([], tf.int64),\n",
    "                        'width': tf.FixedLenFeature([], tf.int64),\n",
    "                        'depth': tf.FixedLenFeature([], tf.int64), }\n",
    "\n",
    "        features = tf.parse_single_example(serialized_example,\n",
    "                                       features=feature_dict)\n",
    "\n",
    "        image_buffer = tf.reshape(features[path], shape=[])\n",
    "        image = tf.decode_raw(image_buffer, tf.uint8)\n",
    "        image = tf.reshape(image, tf.stack([height, width, num_depth]))\n",
    "        image = tf.reshape(image, [1, height, width, num_depth])\n",
    "        image_seq.append(image)    \n",
    "    image_seq = tf.concat(image_seq, 0)\n",
    "    return image_seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_number_of_records(filenames, n_frames):\n",
    "    \"\"\"\n",
    "    this function determines the number of videos available in all tfrecord files. It also checks on the correct shape of the single examples in the tfrecord\n",
    "    files.\n",
    "    :param filenames: a list, each entry containign a (relative) path to one tfrecord file\n",
    "    :return: the number of overall videos provided in the filenames list\n",
    "    \"\"\"\n",
    "\n",
    "    num_examples = 0\n",
    "    if n_frames == 'all':\n",
    "        n_frames_in_test_video = 354\n",
    "    else:\n",
    "        n_frames_in_test_video = n_frames\n",
    "\n",
    "    # create new session to determine batch_size for validation/test data\n",
    "    with tf.Session() as sess_valid:\n",
    "        filename_queue_val = tf.train.string_input_producer(filenames, num_epochs=1)\n",
    "        image_seq_tensor_val = read_and_decode(filename_queue_val, n_frames)\n",
    "\n",
    "        init_op = tf.group(tf.global_variables_initializer(),\n",
    "                           tf.local_variables_initializer())\n",
    "        sess_valid.run(init_op)\n",
    "        coord = tf.train.Coordinator()\n",
    "        threads = tf.train.start_queue_runners(coord=coord)\n",
    "        try:\n",
    "            while True:\n",
    "                video = sess_valid.run([image_seq_tensor_val])\n",
    "                assert np.shape(video) == (1, n_frames_in_test_video, height, width,\n",
    "                                   num_depth), \"shape in the data differs from the expected shape\"\n",
    "                num_examples += 1\n",
    "        except tf.errors.OutOfRangeError as e:\n",
    "            coord.request_stop(e)\n",
    "        finally:\n",
    "            coord.request_stop()\n",
    "            coord.join(threads)\n",
    "    print(num_examples)\n",
    "    return num_examples\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['./data/batch_1_of_1.tfrecords']\n",
      "1\n",
      "1 1 5\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    height = 720\n",
    "    width = 1280\n",
    "    num_depth = 4\n",
    "    out_path = \"./data\"\n",
    "    n_videos_per_record = 1\n",
    "    n_frames = 5\n",
    "    \n",
    "    filenames = gfile.Glob(os.path.join(out_path, \"*.tfrecords\"))\n",
    "    n_files = len(filenames)\n",
    "\n",
    "    print(filenames)\n",
    "    if n_files * n_videos_per_record == get_number_of_records(filenames, n_frames):\n",
    "        print(n_files, n_videos_per_record, n_frames)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:ComputerVision]",
   "language": "python",
   "name": "conda-env-ComputerVision-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
